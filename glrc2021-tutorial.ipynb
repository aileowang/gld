{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSBQlHqRBryF"
   },
   "source": [
    "本代码使用kaggle notebook运行时\n",
    "\n",
    "训练部分使用TPU\n",
    "\n",
    "推断部分使用GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4yV6C8KA87U"
   },
   "source": [
    "# 数据添加\n",
    "https://www.kaggle.com/meminozturk/efficientnetrepo110\n",
    "\n",
    "https://www.kaggle.com/yueqiangqin/kerasapplications\n",
    "\n",
    "https://www.kaggle.com/ragnar123/landmark-image-train\n",
    "\n",
    "https://www.kaggle.com/ragnar123/landmark-tfrecords-384\n",
    "\n",
    "https://www.kaggle.com/ragnar123/landmark-tfrecords-384-2\n",
    "\n",
    "https://www.kaggle.com/c/landmark-recognition-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1YehIZet0aQ"
   },
   "source": [
    "# Google Landmark Recognition 2021\n",
    "随着图像检索和实例识别技术的迅速发展，急需有效的基准数据来对不断出现算法的性能进行有效测评。在这场比赛中，我们通过改进特征提取以及检索算法，给定一张图像，需要在指定图像库识别出所属地标标签并给予评分。\n",
    "\n",
    "所有图像数据是通过网络数据挖掘而来，来自世界各地的景观拍摄，也有部分图片属于非景点，比如人物，车，树木等等；因此数据集非常庞大且含噪声。\n",
    "\n",
    "Google Landmarks Dataset v2(GLD2)含近500万幅图像，训练集4132914幅图像，203094个类别，索引集761757幅图像，测试集117577幅图像，该数据集是由网络数据挖掘而得到的地标图像数据，所以含有非常多的噪声标注。\n",
    "\n",
    "另外该数据集有一个被往届比赛中smlyaka团队使用自动清理工具清理过的版本CGLD2，其训练集含有 1580470 幅图像， 81313 个类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT2eNTMot0aV"
   },
   "source": [
    "## Evaluation 评价指标\n",
    "### GAP(Global Average Precision全局平均精度)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHiD_yz2t0aV"
   },
   "source": [
    "<img src=\"https://latex.codecogs.com/gif.latex?GAP&space;=&space;\\frac{1}{M}\\sum_{i=1}^N&space;P(i)&space;rel(i)\" title=\"GAP = \\frac{1}{M}\\sum_{i=1}^N P(i) rel(i)\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iV0oRewt0aW"
   },
   "source": [
    "真实标签\n",
    "```\n",
    "    id,ground_truth\n",
    "    000088da12d664db,11807\n",
    "    0001623c6d808702,2188\n",
    "    0001bbb682d45002,1054\n",
    "    000270c9100de789,12801\n",
    "    0002b0fab5d3ccc4,NoLandmark\n",
    "```\n",
    "预测值\n",
    "```\n",
    "    id,landmarks\n",
    "    000088da12d664db,11807 0.09\n",
    "    0001623c6d808702,2188 0.13\n",
    "    0001bbb682d45002,7933 0.01\n",
    "    000270c9100de789,12801 0.38\n",
    "    0002b0fab5d3ccc4,6560 0.19\n",
    "```\n",
    "```\n",
    "重排：\n",
    "ranked_predictions = [(12801, 0.38), (6560, 0.19), (2188, 0.13), (11807, 0.09), (7933, 0.01)]\n",
    "确定评价图片数量：\n",
    "N = len(ranked_predictions)\n",
    "M = N - 1 #有一张图片没有标签值\n",
    "计算单个精度：\n",
    "P(1)=1, P(3)=2/3, P(4)=3/4\n",
    "求总值：\n",
    "GAP = 1/M * (P(1) * 1 + P(2) * 0 + P(3) + 1 + P(4) * 1 + P(5) * 0)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6fEaLNKt0aW"
   },
   "source": [
    "## Loss目标函数\n",
    "### Embeddings, Cosine Distance, and ArcFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqHgtVO4t0aX"
   },
   "source": [
    "### Classification CNN\n",
    "普通的图像分类模型一般包括三个模块：输入，中间层，输出。输入就是处理过的图片数据，中间层属于特征提取，输出就是预测分类值。\n",
    "\n",
    "正常来说，我们不需要关注embedding(嵌入向量)，因为一般通过输入图片我们训练一个CNN分类模型，然后得到代表图像类别的one-hot(独热编码)向量。就好比下图，我们训练一个动物分类模型，输入一张图片，然后预测该图片属于动物类别4。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaX-n44Lt0aX"
   },
   "source": [
    "![CNN](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Mar-2021/arcface.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "to05mfNMt0aY"
   },
   "source": [
    "### Embeddings\n",
    "假设现在需要比较两张图片，确认这两张图片是否相似。从机器角度，图片是难以比较的，但是将图片化为数值，那么就简单多了。因此，我们输入一张图片到模型，提取输出层前一层的输出值，这就是图片的嵌入向量。按照上面的模型图，输入需要比较的两张图片，得到两个128维度的表征向量，然后比较这两个向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6DQoqvGt0aZ"
   },
   "source": [
    "### Cosine Distance\n",
    "一般比较两个向量我们可以用计算距离，假如两个向量分别为[0.2, 0.9, 0.7] 、 [0.5, 0.4, 0.1]，那么我们就可以用Euclidean distance(欧几里得距离)计算：\n",
    "``` \n",
    "    sqrt( (0.5-0.2)**2 + (0.4-0.9)**2 + (0.1-0.7)**2 )\n",
    "```\n",
    "余弦相似度，就是计算两个向量间的夹角的余弦值；余弦距离就是用1减去这个获得的余弦相似度，余弦距离的取值范围为[0,2]，数值越大，表明图像越相似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc8VlYAgt0aZ"
   },
   "source": [
    "### ArcFace\n",
    "首先从通用的角度来考虑深度特征学习任务，包括人脸识别、行人再识别、细粒度图像检索等。当给定一个查询图像时，无论图像内容是什么，都希望在库图像中找到一张最相似的候选图像与之匹配。\n",
    "这些特征学习任务具有两个关键要素，分别是特征的类内紧凑度和类间分离度。在特征学习任务中，我们希望优化目标能够最大化类内相似性Sp，同时最小化类间相似性Sn。深度特征学习有两种经典的学习范式。当拥有类别标签时，可以通过使用分类器和分类损失函数来进行训练，从而学习到一个深度特征空间。经典的softmax函数为了向加强类内紧凑度和类间分离度两个方向发展，逐渐演变成ArcFcae,相似的还有CosFace等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRTGxGa0t0aa"
   },
   "source": [
    "![softmax](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Mar-2021/mnist_vgg8_3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56AXI_9Jt0aa"
   },
   "source": [
    "![arcface](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Mar-2021/mnist_vgg8_arcface_3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlAYCE1lt0ab"
   },
   "source": [
    "论文链接：\n",
    "https://arxiv.org/pdf/1801.07698.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7dxnYDit0ac"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import gc\n",
    "import pathlib\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy import spatial\n",
    "import cv2\n",
    "!pip install ../input/kerasapplications/Keras_Applications-1.0.8-py3-none-any.whl\n",
    "!pip install ../input/efficientnetrepo110/efficientnet-1.1.0-py3-none-any.whl\n",
    "import efficientnet.tfkeras as efn\n",
    "import math\n",
    "\n",
    "NUMBER_OF_CLASSES = 81313\n",
    "IMAGE_SIZE = [384, 384]\n",
    "LR = 0.0001\n",
    "\n",
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGkc_Om9t0ad"
   },
   "source": [
    "## Data数据处理(若已生成，可跳过)\n",
    "利用TensorFlow的TFRecord格式，将大量的图片压缩并上传到kaggle数据集系统。\n",
    "\n",
    "* 可以压缩图片存储空间\n",
    "* 便于使用TPU训练\n",
    "* 训练时加快读取数据速度\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nViT9pKt0ad"
   },
   "source": [
    "https://www.tensorflow.org/tutorials/load_data/tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moNuCWxzt0ae"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(feature0, feature1, feature2):\n",
    "    feature = {\n",
    "        'id': _bytes_feature(feature0),\n",
    "        'image': _bytes_feature(feature1),\n",
    "        'target': _int64_feature(feature2)\n",
    "    }\n",
    "    example_proto = tf.train.Example(features = tf.train.Features(feature = feature))\n",
    "    return example_proto.SerializeToString()\n",
    "TRAIN_IMAGE_DIR = '../input/landmark-recognition-2021/train'\n",
    "TRAIN = '../input/landmark-image-train/train_encoded.csv'\n",
    "\n",
    "# Read image and resize it\n",
    "def read_image(image_path, size = (384, 384)):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, size)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_tf_records(record = 0, size = (384, 384)):\n",
    "    df = pd.read_csv(TRAIN)\n",
    "    # Get image paths\n",
    "    image_paths = [x for x in pathlib.Path(TRAIN_IMAGE_DIR).rglob('*.jpg')]\n",
    "    # Get only one group, this is a slow process so we need to make 50 different sessions\n",
    "    df = df[df['group'] == record]\n",
    "    # Reset index \n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    # Get a list of ids\n",
    "    ids_list = list(df['id'].unique())\n",
    "    # Write tf records\n",
    "    with tf.io.TFRecordWriter('train_{}.tfrec'.format(record)) as writer:\n",
    "        for image_path in tqdm(image_paths):\n",
    "            image_id = image_path.name.split('.')[0]\n",
    "            if image_id in ids_list:\n",
    "                # Get target\n",
    "                target = df[df['id'] == image_id]['landmark_id_encode']\n",
    "                img = read_image(str(image_path), size)\n",
    "                img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 100))[1].tostring()\n",
    "                example = serialize_example(\n",
    "                    str.encode(image_id), img, target.values[0]\n",
    "                )\n",
    "                writer.write(example)\n",
    "                \n",
    "#get_tf_records(record = 0, size = (384, 384))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4g2GerHWt0ae"
   },
   "source": [
    "## Model模型构建\n",
    "1. 建立数据读取流水线\n",
    "2. 模型构建\n",
    "3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5mYJEmPt0ae"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.system('pip install -q tensorflow~=2.2.0 tensorflow_gcs_config~=2.2.0')\n",
    "os.system('pip install -q efficientnet')\n",
    "#os.system('pip install -q gcsfs')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import efficientnet.tfkeras as efn\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import requests\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "#resp = requests.post(\"http://{}:8475/requestversion/{}\".format(os.environ[\"COLAB_TPU_ADDR\"].split(\":\")[0], tf.__version__))\n",
    "#if resp.status_code != 200:\n",
    "#    print(\"Failed to switch the TPU to TF {}\".format(version))\n",
    "\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "#\n",
    "# Data access\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path('landmark-tfrecords-384')#'gs://kds-3873db5988cab73fb3f61387f4260e23cbff4012248d13a9fa5dd335'\n",
    "#KaggleDatasets().get_gcs_path('landmark-tfrecords-384')\n",
    "GCS_PATH_2 = KaggleDatasets().get_gcs_path('landmark-tfrecords-384-2')#'gs://kds-e44a20ea6436cd0271b97c5b431450bcd70a463206a983e1acb88874'\n",
    "#KaggleDatasets().get_gcs_path('landmark-tfrecords-384-2')\n",
    "DICT_PATH = KaggleDatasets().get_gcs_path('landmark-image-train')+'/train_encoded.csv'\n",
    "#KaggleDatasets().get_gcs_path('landmark-image-train')\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "IMAGE_SIZE = [384, 384]\n",
    "# Seed\n",
    "SEED = 100\n",
    "# Learning rate\n",
    "LR = 0.0001\n",
    "# Number of classes\n",
    "NUMBER_OF_CLASSES = 81313\n",
    "VALID_STAGE = False\n",
    "# Training filenames directory\n",
    "files1 = tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')\n",
    "files2 = tf.io.gfile.glob(GCS_PATH_2 + '/train*.tfrec')\n",
    "assert len(files1)>0\n",
    "assert len(files2)>0\n",
    "FILENAMES = files1 + files2\n",
    "# Read csv file\n",
    "df = pd.read_csv(DICT_PATH)\n",
    "# Using 20% of the data to validate\n",
    "TRAINING_FILENAMES, VALIDATION_FILENAMES = train_test_split(FILENAMES, test_size = 0.10, random_state = SEED)\n",
    "training_groups = [int(re.compile(r\"_([0-9]*)\\.\").search(filename).group(1)) for filename in TRAINING_FILENAMES]\n",
    "validation_groups = [int(re.compile(r\"_([0-9]*)\\.\").search(filename).group(1)) for filename in VALIDATION_FILENAMES]\n",
    "n_trn_classes = df[df['group'].isin(training_groups)]['landmark_id_encode'].nunique()\n",
    "n_val_classes = df[df['group'].isin(validation_groups)]['landmark_id_encode'].nunique()\n",
    "print(f'The number of unique training classes is {n_trn_classes} of {NUMBER_OF_CLASSES} total classes')\n",
    "print(f'The number of unique validation classes is {n_val_classes} of {NUMBER_OF_CLASSES} total classes')\n",
    "\n",
    "# Seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "# Function to decode our images (normalize and reshape)\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
    "    # Convert image to floats in [0, 1] range\n",
    "    #image = tf.cast(image, tf.float32) #/ 255.0\n",
    "    # Explicit size needed for TPU\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n",
    "\n",
    "# This function parse our images and also get the target variable\n",
    "def read_tfrecord(example):\n",
    "    TFREC_FORMAT = {\n",
    "        # tf.string means bytestring\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
    "        # shape [] means single element\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    target = tf.cast(example['target'], tf.int32)\n",
    "    return image, target\n",
    "\n",
    "# This function load our tf records and parse our data with the previous function\n",
    "def load_dataset(filenames, ordered = False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        # Disable order, increase speed\n",
    "        ignore_order.experimental_deterministic = False \n",
    "    \n",
    "    # Automatically interleaves reads from multiple files\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
    "    # Use data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    # Returns a dataset of (image, label) pairs\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO) \n",
    "    return dataset\n",
    "\n",
    "# This function output the data so that we can use arcface\n",
    "def arcface_format(image, target):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return {'inp1': image, 'inp2': target}, target\n",
    "# augment\n",
    "\n",
    "def augment(image, target):\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image, target\n",
    "\n",
    "\n",
    "# Training data pipeline\n",
    "def get_training_dataset(filenames, ordered = False):\n",
    "    dataset = load_dataset(filenames, ordered = ordered)\n",
    "    dataset = dataset.map(augment, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
    "    # The training dataset must repeat for several epochs\n",
    "    dataset = dataset.repeat() \n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Prefetch next batch while training (autotune prefetch buffer size)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "# Validation data pipeline\n",
    "def get_validation_dataset(filenames, ordered = True, prediction = False):\n",
    "    dataset = load_dataset(filenames, ordered = ordered)\n",
    "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
    "    # If we are in prediction mode, use bigger batch size for faster prediction\n",
    "    if prediction:\n",
    "        dataset = dataset.batch(BATCH_SIZE * 4)\n",
    "    else:\n",
    "        dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Prefetch next batch while training (autotune prefetch buffer size)\n",
    "    dataset = dataset.prefetch(AUTO) \n",
    "    return dataset\n",
    "\n",
    "# Count the number of observations with the tabular csv\n",
    "def count_data_items(filenames):\n",
    "    records = [int(re.compile(r\"_([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    df = pd.read_csv(DICT_PATH)\n",
    "    n = df[df['group'].isin(records)].shape[0]\n",
    "    return n\n",
    "\n",
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "NUM_VALIDATION_IMAGES  = count_data_items(VALIDATION_FILENAMES)\n",
    "print(f'Training with {NUM_TRAINING_IMAGES} images')\n",
    "print(f'Validating with {NUM_VALIDATION_IMAGES} images')\n",
    "\n",
    "\n",
    "tf.keras.utils.get_custom_objects()['ArcMarginProduct'] = ArcMarginProduct\n",
    "# Function to build our model using fine tunning (efficientnet)\n",
    "def get_model():\n",
    "    with strategy.scope():\n",
    "\n",
    "        margin = ArcMarginProduct(\n",
    "            n_classes = NUMBER_OF_CLASSES, \n",
    "            s = 64, \n",
    "            m = 0.2, \n",
    "            name='head/arc_margin', \n",
    "            dtype='float32'\n",
    "            )\n",
    "\n",
    "        inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
    "        label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
    "        x = efn.EfficientNetB5(weights = 'imagenet', include_top = False)(inp)\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(2048)(x)\n",
    "        #x = tf.keras.layers.Dense(2048)(x)\n",
    "        #x = tf.keras.layers.Dense(2048)(x)\n",
    "        x = margin([x, label])\n",
    "    \n",
    "        output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "        model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer = opt,\n",
    "            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "            ) \n",
    "    \n",
    "        return model\n",
    "\n",
    "# Seed everything\n",
    "seed_everything(SEED)\n",
    "\n",
    "# Build training and validation generators\n",
    "train_dataset = get_training_dataset(TRAINING_FILENAMES, ordered = False)\n",
    "val_dataset = get_validation_dataset(VALIDATION_FILENAMES, ordered = True, prediction = False)\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "filepath = f'b5.h5'\n",
    "# Using a checkpoint to save best model (want the entire model, not only the weights)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, \n",
    "                        monitor = 'val_loss', \n",
    "                        save_best_only = True, \n",
    "                        save_weights_only = False)\n",
    "# Using learning rate scheduler\n",
    "cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                            mode = 'min', \n",
    "                            factor = 0.5, \n",
    "                            patience = 1, \n",
    "                            verbose = 1, \n",
    "                            min_delta = 0.0001)\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "# Train and evaluate our model\n",
    "history = model.fit(train_dataset,  \n",
    "            steps_per_epoch = STEPS_PER_EPOCH,\n",
    "            epochs = EPOCHS,\n",
    "            callbacks = [cb_lr_schedule, checkpoint],\n",
    "            validation_data = val_dataset,\n",
    "            verbose = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XhDM2Mlt0ah"
   },
   "source": [
    "## Inference推断阶段\n",
    "* 构建测试集与训练集图片嵌入向量索引\n",
    "* 相似图片排序\n",
    "* 返回最高分图片标签\n",
    "* 输出标签与得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pZl94O3t0ai"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import gc\n",
    "import pathlib\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy import spatial\n",
    "import cv2\n",
    "!pip install ../input/kerasapplications/Keras_Applications-1.0.8-py3-none-any.whl\n",
    "!pip install ../input/efficientnetrepo110/efficientnet-1.1.0-py3-none-any.whl\n",
    "import efficientnet.tfkeras as efn\n",
    "import math\n",
    "\n",
    "NUMBER_OF_CLASSES = 81313\n",
    "IMAGE_SIZE = [384, 384]\n",
    "LR = 0.0001\n",
    "\n",
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "            blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output\n",
    "\n",
    "\n",
    "# Function to build our model using fine tunning (efficientnet)\n",
    "def get_model():\n",
    "\n",
    "    margin = ArcMarginProduct(\n",
    "        n_classes = NUMBER_OF_CLASSES, \n",
    "        s = 30, \n",
    "        m = 0.5, \n",
    "        name='head/arc_margin', \n",
    "        dtype='float32'\n",
    "        )\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
    "    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
    "    x0 = efn.EfficientNetB5(weights = None, include_top = False)(inp)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x0)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(2048)(x) #-4\n",
    "    x = margin([x, label])\n",
    "\n",
    "    output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = opt,\n",
    "        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "        ) \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "NUM_EMBEDDING_DIMENSIONS = 2048\n",
    "DATASET_DIR = '../input/landmark-image-train/train_encoded.csv'\n",
    "TEST_IMAGE_DIR = '../input/landmark-recognition-2021/test'\n",
    "TRAIN_IMAGE_DIR = '../input/landmark-recognition-2021/train'\n",
    "MODEL = get_model()\n",
    "\n",
    "MODEL = tf.keras.models.Model(inputs = MODEL.input[0], outputs = MODEL.layers[-4].output)\n",
    "MODEL.load_weights('b5.h5')\n",
    "NUM_TO_RERANK = 3\n",
    "\n",
    "\n",
    "NUM_PUBLIC_TEST_IMAGES = 10345 # Used to detect if in session or re-run.\n",
    "\n",
    "# Read image and resize it\n",
    "def read_image(image_path, size = (384, 384)):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, size)\n",
    "    img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 100))[1].tostring()\n",
    "    img = tf.image.decode_jpeg(img, channels = 3)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    img = tf.reshape(img, [1, 384, 384, 3])\n",
    "    return img\n",
    "\n",
    "# Function to get training and test embeddings\n",
    "def generate_embeddings(filepaths):\n",
    "    image_paths = [x for x in pathlib.Path(filepaths).rglob('*.jpg')]\n",
    "    num_images = len(image_paths)\n",
    "    ids = num_images * [None]\n",
    "    # Generate an empty matrix where we can store the embeddings of each image\n",
    "    embeddings = np.empty((num_images, NUM_EMBEDDING_DIMENSIONS))\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        ids[i] = image_path.name.split('.')[0]\n",
    "        image_tensor = read_image(str(image_path), (384, 384))\n",
    "        prediction = MODEL.predict(image_tensor) # 2048d\n",
    "        embeddings[i, :] = tf.nn.l2_normalize(prediction, axis=1, epsilon=1e-12, name=None)#prediction\n",
    "    return ids, embeddings\n",
    "\n",
    "# This function get the most similar train images for each test image based on cosine similarity\n",
    "def get_similarities(train_csv, test_directory, train_directory):\n",
    "    # Get target dictionary\n",
    "    df = pd.read_csv(train_csv)\n",
    "    df = df[['id', 'landmark_id']]\n",
    "    df.set_index('id', inplace = True)\n",
    "    df = df.to_dict()['landmark_id']\n",
    "    # Extract the test ids and global feature for the test images\n",
    "    test_ids, test_embeddings = generate_embeddings(test_directory) #test \n",
    "    # Extract the train ids and global features for the train images\n",
    "    train_ids, train_embeddings = generate_embeddings(train_directory) \n",
    "    # Initiate a list were we will store the similar training images for each test image (also score)\n",
    "    train_ids_labels_and_scores = [None] * test_embeddings.shape[0]\n",
    "    # Using (slow) for-loop, as distance matrix doesn't fit in memory\n",
    "    for test_index in range(test_embeddings.shape[0]):\n",
    "        distances = spatial.distance.cdist(\n",
    "            test_embeddings[np.newaxis, test_index, : ], train_embeddings, 'cosine')[0]\n",
    "        # Get the indices of the closest images\n",
    "        top_k = np.argpartition(distances, NUM_TO_RERANK)[:NUM_TO_RERANK]\n",
    "        # Get the nearest ids and distances using the previous indices\n",
    "        nearest = sorted([(train_ids[p], distances[p]) for p in top_k], key = lambda x: x[1])\n",
    "        # Get the labels and score results\n",
    "        train_ids_labels_and_scores[test_index] = [(df[train_id], 1.0 - cosine_distance) for \\\n",
    "                                                   train_id, cosine_distance in nearest]\n",
    "        \n",
    "    del test_embeddings\n",
    "    del train_embeddings\n",
    "    gc.collect()\n",
    "    return test_ids, train_ids_labels_and_scores\n",
    "\n",
    "# This function aggregate top simlarities and make predictions\n",
    "def generate_predictions(test_ids, train_ids_labels_and_scores):\n",
    "    targets = []\n",
    "    scores = []\n",
    "    \n",
    "    # Iterate through each test id\n",
    "    for test_index, test_id in enumerate(test_ids):\n",
    "        aggregate_scores = {}\n",
    "        # Iterate through the similar images with their corresponing score for the given test image\n",
    "        for target, score in train_ids_labels_and_scores[test_index]:\n",
    "            if target not in aggregate_scores:\n",
    "                aggregate_scores[target] = 0\n",
    "            aggregate_scores[target] += score\n",
    "        # Get the best score\n",
    "        target, score = max(aggregate_scores.items(), key = operator.itemgetter(1))\n",
    "        targets.append(target)\n",
    "        scores.append(score)\n",
    "        \n",
    "    final = pd.DataFrame({'id': test_ids, 'target': targets, 'scores': scores})\n",
    "    final['landmarks'] = final['target'].astype(str) + ' ' + final['scores'].astype(str)\n",
    "    final[['id', 'landmarks']].to_csv('submission.csv', index = False)\n",
    "    return final\n",
    "\n",
    "def inference_and_save_submission_csv(train_csv, test_directory, train_directory):\n",
    "    image_paths = [x for x in pathlib.Path(test_directory).rglob('*.jpg')]\n",
    "    test_len = len(image_paths)\n",
    "    if test_len == NUM_PUBLIC_TEST_IMAGES:\n",
    "        # Dummy submission\n",
    "        shutil.copyfile('../input/landmark-recognition-2021/sample_submission.csv', 'submission.csv')\n",
    "        return 'Job Done'\n",
    "    else:\n",
    "        test_ids, train_ids_labels_and_scores = get_similarities(train_csv, test_directory, train_directory)\n",
    "        final = generate_predictions(test_ids, train_ids_labels_and_scores)\n",
    "        return final\n",
    "    \n",
    "final = inference_and_save_submission_csv(DATASET_DIR, TEST_IMAGE_DIR, TRAIN_IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHBjB2GIt0aj"
   },
   "source": [
    "# 总结\n",
    "本解决方案基于度量学习来对众多地标类别进行分类，分别利用Efficientnet-b7与Efficientnet-b6在imagenet的预训练权重，在超大地标图像数据集重新训练，进一步在较大尺寸图像上微调，调整ArcFace的超平面参数，有效获取图像嵌入空间。在得到指定图像库与待识别图像库嵌入向量后，逐一计算两个图库图像余弦距离，利用余弦距离进一步重新排序获取最高者的标签索引与分数，生成最终结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcRFEWeTt0aj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "glrc2021-tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
